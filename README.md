# Legion: Distributed LLM Training

> _A SETI@home for training language models - distributed pre-training across the internet_

## Overview

Legion is an experimental distributed training system that aims to enable LLM pre-training across consumer-grade machines. Inspired by SETI@home, it explores whether modern distributed training techniques (ZeRO, gradient compression, fault tolerance) can work over high-latency, low-bandwidth consumer networks.

See [PROJECT.md](PROJECT.md) for the complete project plan and technical details.

## Current Status: Phase 1.3 Complete - Real Distributed Training

Legion has completed Phase 0 (simulation) and Phase 1.3 (distributed infrastructure) with working multi-worker training:

**Phase 0 Complete:**
- âœ… Parameter partitioning (ZeRO-3 style)
- âœ… Collective communication (all-gather, reduce-scatter)
- âœ… Gradient compression (INT8 quantization)
- âœ… Network latency simulation
- âœ… End-to-end training test

**Phase 1.3 Complete:**
- âœ… Coordinator server (REST + WebSocket)
- âœ… Worker client with heartbeat and telemetry
- âœ… gRPC worker-to-worker communication
- âœ… **Real distributed training with ZeRO-3 across multiple machines**
- âœ… Gradient accumulation and synchronization
- âœ… Parameter exchange via gRPC all-gather and reduce-scatter
- âœ… Multi-worker integration tests (2+ workers verified)
- âœ… HuggingFace dataset integration (FineWeb, The Pile, Shakespeare, etc.)
- âœ… Proper data parallelism with dataset sharding
- âœ… Async collective operations for improved overlap
- âœ… Version manager for model checkpoint coordination
- âœ… Work stealing infrastructure for fault tolerance

**Next Steps (Phase 2):**
- Add compression to gRPC transfers (INT8, TopK)
- Latency measurement and regional clustering
- Enhanced fault tolerance testing (worker dropout/rejoin)
- Async gradient accumulation with variable worker participation
- Scale to 4-8 workers for performance validation

## Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/zhubert/legion.git
cd legion

# Create virtual environment
python -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows

# Install dependencies
pip install -r requirements.txt
```

### Running the Simulation

```bash
# Run single-machine simulation with 4 workers
python sim/train.py --workers 4 --model tiny

# With latency simulation (50ms)
python sim/train.py --workers 4 --model tiny --latency 50

# With compression enabled
python sim/train.py --workers 4 --model tiny --compress int8
```

### Running Distributed Training

**Important:** The coordinator controls all training configuration (dataset, model, hyperparameters). Workers automatically fetch and execute the coordinator's decisions.

**Option 1: One-Command Orchestrator (Recommended)**
```bash
# Start all services (coordinator + 2 workers + assembler) in one terminal
python scripts/start_services.py

# With log files
python scripts/start_services.py --logs-dir logs

# Custom number of workers
python scripts/start_services.py --workers 3

# Skip assembler service
python scripts/start_services.py --no-assembler
```

This orchestrator:
- Starts coordinator, workers, and checkpoint assembler
- Color-codes output per service for easy reading
- Handles graceful shutdown with Ctrl+C
- Optionally writes logs to separate files
- Shows unified, timestamped output from all services

**Option 2: Automated 2-Worker Test**
```bash
# Terminal 1: Start coordinator (uses default config)
python -m coordinator.server

# Terminal 2: Run automated test
python scripts/test_two_workers.py
```

This script will:
- Start 2 workers automatically
- Workers fetch training config from coordinator
- Run 50 training steps with real distributed training
- Verify gradient synchronization and parameter exchange
- Report performance metrics and loss convergence

**Option 3: Manual Multi-Worker Setup with Custom Configuration**
```bash
# Terminal 1: Start coordinator
python -m coordinator.server
# Server runs on http://localhost:8000

# Terminal 2: Configure training (optional - coordinator has sensible defaults)
curl -X PUT http://localhost:8000/training/config \
  -H "Content-Type: application/json" \
  -d '{"dataset_type": "distributed_dummy", "batch_size": 4, "num_steps": 100}'

# Terminal 3: Start worker 1 (automatically fetches config from coordinator)
python -m worker.client

# Terminal 4: Start worker 2 (automatically fetches config from coordinator)
python -m worker.client
```

Workers will automatically:
- Register with the coordinator
- **Fetch training configuration from coordinator**
- Send periodic heartbeats
- Wait for peers to be ready
- Form a training cluster
- Execute training with coordinator's configuration
- Exchange parameters via gRPC
- Synchronize gradients across workers

### Training Configuration (Coordinator-Driven)

**Key Design Principle:** The coordinator makes all training decisions (dataset, model, hyperparameters). Workers simply execute the coordinator's configuration.

**Setting Training Configuration:**

```bash
# Option 1: Start coordinator with default config (distributed_dummy dataset)
python -m coordinator.server

# Option 2: Configure via API after startup
curl -X PUT http://localhost:8000/training/config \
  -H "Content-Type: application/json" \
  -d '{
    "dataset_type": "huggingface",
    "dataset_name": "tiny_shakespeare",
    "model_size": "tiny",
    "batch_size": 8,
    "seq_len": 256,
    "num_steps": 100
  }'

# Option 3: Use Python to configure
python -c "
import requests
requests.put('http://localhost:8000/training/config', json={
    'dataset_type': 'huggingface',
    'dataset_name': 'fineweb-edu',
    'batch_size': 8,
    'seq_len': 1024,
    'num_steps': 1000
})
"
```

**Available datasets:**
- `fineweb` - 15T tokens from CommonCrawl
- `fineweb-edu` - 1.3T high-quality educational tokens
- `pile` - 825GB diverse dataset
- `tiny_shakespeare` - 1MB for testing
- `shakespeare` - Complete works of Shakespeare
- `distributed_dummy` - Synthetic data for testing (default)

**Workers automatically receive configuration from coordinator:**
```bash
# Workers no longer need dataset/model flags - they fetch config from coordinator
python -m worker.client
```

## Project Structure

```
legion/
â”œâ”€â”€ PROJECT.md              # Detailed project plan
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ CLAUDE.md               # Development guide
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ core/                   # Shared core functionality
â”‚   â”œâ”€â”€ model.py            # Model definitions (TinyGPT)
â”‚   â”œâ”€â”€ partitioner.py      # ZeRO-3 parameter partitioning
â”‚   â”œâ”€â”€ compression.py      # Gradient compression (INT8, TopK)
â”‚   â””â”€â”€ dataset.py          # Dataset utilities (HuggingFace integration)
â”œâ”€â”€ sim/                    # Phase 0: Single-machine simulation
â”‚   â”œâ”€â”€ collectives.py      # Shared-memory collectives
â”‚   â”œâ”€â”€ worker.py           # Simulated worker coordinator
â”‚   â””â”€â”€ train.py            # Simulation entry point
â”œâ”€â”€ coordinator/            # Phase 1: Central coordinator
â”‚   â”œâ”€â”€ server.py           # FastAPI REST + WebSocket server
â”‚   â”œâ”€â”€ registry.py         # Worker registration and health
â”‚   â”œâ”€â”€ clustering.py       # Latency-based regional clustering
â”‚   â”œâ”€â”€ database.py         # SQLite persistence
â”‚   â””â”€â”€ version_manager.py  # Model checkpoint version tracking
â”œâ”€â”€ worker/                 # Phase 1: Distributed worker nodes
â”‚   â”œâ”€â”€ client.py           # Main worker orchestration
â”‚   â”œâ”€â”€ coordinator_client.py  # HTTP client for coordinator
â”‚   â”œâ”€â”€ heartbeat.py        # Periodic heartbeat manager
â”‚   â”œâ”€â”€ trainer.py          # Distributed training loop
â”‚   â”œâ”€â”€ shard_manager.py    # Parameter shard management
â”‚   â””â”€â”€ telemetry.py        # Metrics reporting
â”œâ”€â”€ communication/          # Phase 1: Worker-to-worker gRPC
â”‚   â”œâ”€â”€ grpc_server.py      # gRPC server for parameters and gradients
â”‚   â”œâ”€â”€ grpc_client.py      # gRPC client for parameter exchange
â”‚   â”œâ”€â”€ collectives.py      # Shared-memory collective operations
â”‚   â”œâ”€â”€ async_collectives.py # Async all-gather/reduce-scatter
â”‚   â”œâ”€â”€ serialization.py    # Tensor serialization/chunking
â”‚   â””â”€â”€ proto/              # Protocol buffer definitions
â””â”€â”€ tests/                  # Comprehensive test suite
    â”œâ”€â”€ integration/        # Multi-worker integration tests
    â”œâ”€â”€ test_*.py           # Unit tests
    â””â”€â”€ ...                 # 168 tests total
```

## Key Concepts

### Parameter Partitioning (ZeRO-3)

Each worker owns a subset of model parameters. During training:

- **All-gather**: Workers collect parameters from others for forward/backward pass
- **Reduce-scatter**: Gradients are aggregated and sent back to parameter owners
- **Update**: Only owners update their parameters

This reduces memory usage from `O(model_size)` to `O(model_size / num_workers)` per worker.

### Gradient Compression

Gradients are compressed before transmission:

- **INT8 quantization**: 4x compression (FP32 â†’ INT8)
- **TopK sparsification**: Send only largest gradients
- **1-bit Adam** (planned): 32x compression after warmup
- **Target**: 64-100x total compression

### Async Collective Operations

Legion uses asynchronous collective operations for improved overlap and efficiency:

- **Async all-gather**: Non-blocking parameter collection from peers
- **Async reduce-scatter**: Overlapped gradient aggregation and distribution
- **Background I/O**: Communication overlaps with computation
- **Future-based API**: Enables pipeline parallelism across training steps

This design allows workers to hide communication latency behind computation.

### Network Architecture

**Coordinator** (FastAPI server):
- Worker registration and health monitoring
- Regional clustering based on latency
- Metrics aggregation and version tracking
- Model checkpoint coordination
- NOT in the training loop (peer-to-peer communication)

**Workers** (async Python clients):
- gRPC server for serving parameter shards
- gRPC client for fetching from peers
- Training loop with ZeRO-3 partitioning
- Automatic fault detection and recovery
- Work stealing for load balancing

## Testing

Run the comprehensive test suite:

```bash
# All tests
pytest

# With coverage
pytest --cov=sim --cov=coordinator --cov=worker --cov=communication

# Integration tests only
pytest tests/integration/

# Specific test file
pytest tests/test_async_collectives.py -v
```

**Test Coverage:**
- 168 total tests (164 passing, 4 skipped)
- Unit tests for all components
- Integration tests for multi-worker scenarios
- End-to-end distributed training tests
- gRPC communication tests
- Async collectives tests
- Version manager and work stealing tests

## Contributing

This is an early-stage research project. Contributions are welcome!

See [CLAUDE.md](CLAUDE.md) for development guidance.

## License

MIT License - See [LICENSE](LICENSE)

## Performance Characteristics

Current implementation benchmarks:

**Communication:**
- gRPC parameter transfer: Supports 100MB+ messages
- Tensor serialization: NumPy-based with chunking
- Async collectives: Non-blocking all-gather and reduce-scatter

**Memory:**
- ZeRO-3 partitioning: `O(model_size / num_workers)` per worker
- Parameter shards saved to disk for checkpointing
- Gradient accumulation for large batches

**Scalability:**
- Tested: 2 workers with real distributed training (verified working)
- Ready for: 4-8 workers multi-machine
- Designed for: 8-32 workers globally
- Target: 100+ workers with regional clustering

## Resources

**Papers:**
- [DeepSpeed ZeRO](https://arxiv.org/abs/1910.02054) - Parameter partitioning
- [1-bit Adam](https://arxiv.org/abs/2102.02888) - Gradient compression
- [PyTorch FSDP](https://arxiv.org/abs/2304.11277) - Async collective operations

**Documentation:**
- [DeepSpeed](https://www.deepspeed.ai/)
- [gRPC Python](https://grpc.io/docs/languages/python/)
- [FastAPI](https://fastapi.tiangolo.com/)

---

_Let's democratize AI training, one GPU at a time._ ðŸš€
