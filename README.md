# Legion: Distributed LLM Training

> _A SETI@home for training language models - distributed pre-training across the internet_

## Overview

Legion is an experimental distributed training system that aims to enable LLM pre-training across consumer-grade machines. Inspired by SETI@home, it explores whether modern distributed training techniques (ZeRO, gradient compression, fault tolerance) can work over high-latency, low-bandwidth consumer networks.

See [PROJECT.md](PROJECT.md) for the complete project plan and technical details.

## Current Status: Phase 1 - Core Infrastructure

Legion has completed the proof-of-concept simulation (Phase 0) and is now in Phase 1 with functional distributed infrastructure:

**Phase 0 Complete:**
- âœ… Parameter partitioning (ZeRO-3 style)
- âœ… Collective communication (all-gather, reduce-scatter)
- âœ… Gradient compression (INT8 quantization)
- âœ… Network latency simulation
- âœ… End-to-end training test

**Phase 1 Complete:**
- âœ… Coordinator server (REST + WebSocket)
- âœ… Worker client with heartbeat and telemetry
- âœ… gRPC worker-to-worker communication
- âœ… Ring-based collectives (8x-512x bandwidth savings)
- âœ… Multi-worker integration tests

**Next Steps (Phase 1 Remaining):**
- Real multi-machine distributed training (2-4 workers)
- Latency measurement and regional clustering
- Fault tolerance testing

## Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/zhubert/legion.git
cd legion

# Create virtual environment
python -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows

# Install dependencies
pip install -r requirements.txt
```

### Running the Simulation

```bash
# Run single-machine simulation with 4 workers
python sim/train.py --workers 4 --model tiny

# With latency simulation (50ms)
python sim/train.py --workers 4 --model tiny --latency 50

# With compression enabled
python sim/train.py --workers 4 --model tiny --compress int8
```

### Running Distributed Training

**Terminal 1: Start the coordinator server**
```bash
python -m coordinator.server
# Server runs on http://localhost:8000
```

**Terminal 2+: Start worker nodes**
```bash
# Worker 1
python -m worker.client

# Worker 2 (in another terminal)
python -m worker.client
```

Workers will automatically:
- Register with the coordinator
- Send periodic heartbeats
- Form a training cluster
- Exchange parameters via gRPC

## Project Structure

```
legion/
â”œâ”€â”€ PROJECT.md              # Detailed project plan
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ CLAUDE.md               # Development guide
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ sim/                    # Phase 0: Single-machine simulation
â”‚   â”œâ”€â”€ model.py            # Tiny transformer for testing
â”‚   â”œâ”€â”€ partitioner.py      # Parameter partitioning (ZeRO-3)
â”‚   â”œâ”€â”€ collectives.py      # All-gather, reduce-scatter
â”‚   â”œâ”€â”€ compression.py      # Gradient compression
â”‚   â”œâ”€â”€ worker.py           # Simulated worker coordinator
â”‚   â””â”€â”€ train.py            # Simulation entry point
â”œâ”€â”€ coordinator/            # Phase 1: Central coordinator
â”‚   â”œâ”€â”€ server.py           # FastAPI REST + WebSocket server
â”‚   â”œâ”€â”€ registry.py         # Worker registration and health
â”‚   â”œâ”€â”€ clustering.py       # Latency-based regional clustering
â”‚   â””â”€â”€ database.py         # SQLite persistence
â”œâ”€â”€ worker/                 # Phase 1: Distributed worker nodes
â”‚   â”œâ”€â”€ client.py           # Main worker orchestration
â”‚   â”œâ”€â”€ coordinator_client.py  # HTTP client for coordinator
â”‚   â”œâ”€â”€ heartbeat.py        # Periodic heartbeat manager
â”‚   â”œâ”€â”€ trainer.py          # Distributed training loop
â”‚   â”œâ”€â”€ shard_manager.py    # Parameter shard management
â”‚   â””â”€â”€ telemetry.py        # Metrics reporting
â”œâ”€â”€ communication/          # Phase 1: Worker-to-worker gRPC
â”‚   â”œâ”€â”€ grpc_server.py      # gRPC server for parameters
â”‚   â”œâ”€â”€ grpc_client.py      # gRPC client for requests
â”‚   â”œâ”€â”€ grpc_collectives.py # gRPC-based all-gather/reduce-scatter
â”‚   â”œâ”€â”€ ring_collectives.py # Ring-based bandwidth optimization
â”‚   â”œâ”€â”€ serialization.py    # Tensor serialization/chunking
â”‚   â””â”€â”€ proto/              # Protocol buffer definitions
â””â”€â”€ tests/                  # Comprehensive test suite
    â”œâ”€â”€ integration/        # Multi-worker integration tests
    â”œâ”€â”€ test_*.py           # Unit tests
    â””â”€â”€ ...                 # 147 tests total
```

## Key Concepts

### Parameter Partitioning (ZeRO-3)

Each worker owns a subset of model parameters. During training:

- **All-gather**: Workers collect parameters from others for forward/backward pass
- **Reduce-scatter**: Gradients are aggregated and sent back to parameter owners
- **Update**: Only owners update their parameters

This reduces memory usage from `O(model_size)` to `O(model_size / num_workers)` per worker.

### Gradient Compression

Gradients are compressed before transmission:

- **INT8 quantization**: 4x compression (FP32 â†’ INT8)
- **TopK sparsification**: Send only largest gradients
- **1-bit Adam** (planned): 32x compression after warmup
- **Target**: 64-100x total compression

### Ring-Based Collectives

Bandwidth-efficient communication pattern where each worker only talks to 2 neighbors:

```
Worker topology: 0 <-> 1 <-> 2 <-> 3 <-> 0
```

**Bandwidth savings vs naive all-to-all:**
- 4 workers: 8x reduction
- 8 workers: 32x reduction
- 16 workers: 128x reduction
- 32 workers: 512x reduction

Example: For a 1B parameter model (4GB) on 16 workers:
- Ring all-reduce: ~7.75 GB total communication
- Naive all-reduce: ~960 GB total communication
- **128x improvement!**

### Network Architecture

**Coordinator** (FastAPI server):
- Worker registration and health monitoring
- Regional clustering based on latency
- Metrics aggregation
- NOT in the training loop (peer-to-peer communication)

**Workers** (async Python clients):
- gRPC server for serving parameter shards
- gRPC client for fetching from peers
- Training loop with ZeRO-3 partitioning
- Automatic fault detection and recovery

## Testing

Run the comprehensive test suite:

```bash
# All tests
pytest

# With coverage
pytest --cov=sim --cov=coordinator --cov=worker --cov=communication

# Integration tests only
pytest tests/integration/

# Specific test file
pytest tests/test_ring_collectives.py -v
```

**Test Coverage:**
- 147 total tests (146 passing, 1 requires running coordinator)
- Unit tests for all components
- Integration tests for multi-worker scenarios
- gRPC communication tests
- Ring collectives performance tests

## Contributing

This is an early-stage research project. Contributions are welcome!

See [CLAUDE.md](CLAUDE.md) for development guidance.

## License

MIT License - See [LICENSE](LICENSE)

## Performance Characteristics

Current implementation benchmarks:

**Communication:**
- gRPC parameter transfer: Supports 100MB+ messages
- Tensor serialization: NumPy-based with chunking
- Ring all-reduce: O(N) steps, O(1) bandwidth per worker

**Memory:**
- ZeRO-3 partitioning: `O(model_size / num_workers)` per worker
- Parameter shards saved to disk for checkpointing
- Gradient accumulation for large batches

**Scalability:**
- Tested: 2-4 workers locally
- Designed for: 8-32 workers globally
- Target: 100+ workers with regional clustering

## Resources

**Papers:**
- [DeepSpeed ZeRO](https://arxiv.org/abs/1910.02054) - Parameter partitioning
- [1-bit Adam](https://arxiv.org/abs/2102.02888) - Gradient compression
- [Ring All-Reduce](https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/) - Bandwidth optimization

**Documentation:**
- [DeepSpeed](https://www.deepspeed.ai/)
- [gRPC Python](https://grpc.io/docs/languages/python/)
- [FastAPI](https://fastapi.tiangolo.com/)

---

_Let's democratize AI training, one GPU at a time._ ðŸš€
